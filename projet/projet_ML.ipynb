{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf17194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2cc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Etire le vecteur v pour obtenir une matrice de taille n*taille_de_v dont chaque ligne est une copie de v\"\"\"\n",
    "def stretch_vect(v, n):\n",
    "    return [v for i in range(n)]\n",
    "\n",
    "\"\"\"renvoie le produit des matrices m1 et m2 (terme à terme)\"\"\"\n",
    "def prod_mat(m1, m2):\n",
    "    n, m = m1.shape\n",
    "    res = np.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            res[i][j] = m1[i][j]*m2[i][j]\n",
    "    return res\n",
    "\n",
    "\"\"\"renvoie la division des matrices m1 et m2 (terme à terme)\"\"\"\n",
    "def div_mat(m1, m2):\n",
    "    n, m = m1.shape\n",
    "    res = np.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            res[i][j] = m1[i][j]/m2[i][j]\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1795186f",
   "metadata": {},
   "source": [
    "# 1. Mon premier est ... linéaire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25050a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Loss(object):\n",
    "    def forward(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    def backward(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    def forward(self, y, yhat):\n",
    "        return np.mean(np.power(y-yhat, 2), axis=1)\n",
    "\n",
    "    def backward(self, y, yhat):\n",
    "        return 2*(yhat-y)/len(y)\n",
    "\n",
    "\n",
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self._parameters = None\n",
    "        self._gradient = None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        ## Annule gradient\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        ## Calcule la passe forward\n",
    "        pass\n",
    "\n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        ## Calcule la mise a jour des parametres selon le gradient calcule et le pas de gradient_step\n",
    "        self._parameters -= gradient_step*self._gradient\n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        ## Met a jour la valeur du gradient\n",
    "        pass\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcul la derivee de l'erreur\n",
    "        pass\n",
    "    \n",
    "class LinearModule(Module):\n",
    "    \"\"\"\n",
    "    N : la taille du mini-batch\n",
    "    d : le nombre de dimensions d'un échantillon\n",
    "    d' : le nombre de neurones dans le module\"\"\"\n",
    "    def __init__(self, d, dprime, N):\n",
    "        Module.__init__(self)\n",
    "        self.W = np.random.rand(d, dprime) #W est de taille d*d'\n",
    "        self.b = stretch_vect(np.random.rand(1, dprime), N) #b est de taille N*d' (on réplique N fois la première ligne)\n",
    "        self.gradientW = None\n",
    "        self.gradientb = None\n",
    "\n",
    "    \"\"\"def zero_grad(self):\n",
    "        ## Annule gradient\n",
    "        #TODO\n",
    "        self.gradientW = None\n",
    "        self.gradientb = None\"\"\"\n",
    "\n",
    "    def forward(self, X):\n",
    "        ## Calcule la passe forward\n",
    "        self.input = X\n",
    "        return X@self.W + self.b\n",
    "\n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        ## Calcule la mise a jour des parametres selon le gradient calcule et le pas de gradient_step\n",
    "        self.W -= gradient_step*self.gradientW\n",
    "        self.b -= gradient_step*self.gradientb\n",
    "        self._parameters = [self.W, self.b]\n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        ## Calcule le gradient du coût par rapport aux paramètres et l’additionne au gradient (gradientW et gradientb)\n",
    "        # - en fonction de l’entrée input et des δ de la couche suivante delta\n",
    "        # Fait une moyenne sur les échantillons\n",
    "        N = len(input)\n",
    "        self.gradientW = input.T@stretch_vect(delta, N)/N\n",
    "        self.gradientb = delta\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcule le gradient du coût par rapport aux entrées en fonction de l’entrée \"input\" et des deltas de la couche suivante \"delta\"\n",
    "        return delta@self.X.T\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b40dd903",
   "metadata": {},
   "source": [
    "## Tests module linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3847711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = LinearModule()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd951fc8",
   "metadata": {},
   "source": [
    "# 2. Mon second est ... non-linéaire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f53c46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        #self.b = stretch_vect(np.random.rand(1, d), N) #TODO y a bien un biais\n",
    "\n",
    "    \"\"\"def zero_grad(self):\n",
    "        ## Annule gradient\n",
    "        #TODO\n",
    "        self.gradientW = None\n",
    "        self.gradientb = None\"\"\"\n",
    "\n",
    "    def forward(self, X):\n",
    "        ## Calcule la passe forward\n",
    "        self.input = X\n",
    "        return np.tanh(X) #+self.b\n",
    "\n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        ## il n'y a pas de paramètres à mettre a jour\n",
    "        pass #TODO\n",
    "        \n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        pass\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcule le gradient du coût par rapport aux entrées en fonction de l’entrée \"input\" et des deltas de la couche suivante \"delta\"\n",
    "        return prod_mat(stretch_vect(delta, len(input)),np.pow(1-np.tanh(input), 2))\n",
    "    \n",
    "class Sigmoide(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        #self.b = stretch_vect(np.random.rand(1, d), N)\n",
    "\n",
    "    \"\"\"def zero_grad(self):\n",
    "        ## Annule gradient\n",
    "        #TODO\n",
    "        self.gradientW = None\n",
    "        self.gradientb = None\"\"\"\n",
    "\n",
    "    def forward(self, X):\n",
    "        ## Calcule la passe forward\n",
    "        self.input = X\n",
    "        return 1/(1 + np.exp(-X)) #+self.b #il faut que X soit un np array (-X)\n",
    "\n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        ## il n'y a pas de paramètres à mettre a jour\n",
    "        pass #TODO\n",
    "        \n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        pass\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcule le gradient du coût par rapport aux entrées en fonction de l’entrée \"input\" et des deltas de la couche suivante \"delta\"\n",
    "        return prod_mat(stretch_vect(delta, len(input)),div_mat(np.exp(-input), 1+np.exp(-input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentiel(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "class Optim(object):\n",
    "    def __init__(self, net, loss, eps):\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.eps = eps\n",
    "    \n",
    "    def step(self, batch_x, batch_y):\n",
    "        output = batch_x\n",
    "        for layer in self.net.layers:\n",
    "            output = layer.forward(output)\n",
    "        cost = self.loss.forward(batch_y, output)\n",
    "        delta = self.loss.backward(batch_y, output)\n",
    "        for layer in reversed(self.net.layers):\n",
    "            layer.backward_update_gradient(layer.input, delta)\n",
    "            error = layer.backward_delta(layer.input, error)\n",
    "        #TODO quand est ce qu on update les parametres ?\n",
    "        for layer in self.net.layers:\n",
    "            layer.update_parameters(gradient_step=self.eps)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
